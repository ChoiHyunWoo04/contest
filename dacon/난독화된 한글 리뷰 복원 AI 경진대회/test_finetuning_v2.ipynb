{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "263bf8d5-887e-40d7-9c09-d6a64b252bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f6b8c1-1384-4fc7-9b47-2e150f7a42c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134e408799c24fbab315663d21014bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_MODEL = \"beomi/gemma-ko-7b\"\n",
    "FINETUNE_MODEL = \"./gemma-ko-7b-finetuning-decoder-with-shuffle\"\n",
    "\n",
    "finetune_model = AutoModelForCausalLM.from_pretrained(FINETUNE_MODEL, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74cb9882-907e-48dd-a56f-fe69ab01ab47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(finetune_model.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7654b1-5046-4c9d-abff-a40bda0d5a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9517a855-256e-47c3-aa36-2cfb32076d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe_finetuned = pipeline(task=\"text-generation\", model=finetune_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b87a64a-2a7b-4d1b-bfa0-56ef886f8a9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 14\u001b[0m\n\u001b[0;32m      6\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m<bos><start_of_turn>user\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m글자 수와 띄어쓰기를 유지하며 리뷰를 복원하세요:\u001b[39m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m<end_of_turn>\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m<start_of_turn>model\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(query))\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# 텍스트 생성\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     generated \u001b[38;5;241m=\u001b[39m pipe_finetuned(\n\u001b[0;32m     15\u001b[0m         prompt,\n\u001b[0;32m     16\u001b[0m         num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     17\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m     18\u001b[0m         top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[0;32m     19\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(query) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     20\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[0;32m     22\u001b[0m     )\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# 생성된 텍스트에서 출력 부분 추출\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m generated[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:285\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    284\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\pipelines\\base.py:1362\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1355\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1356\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         )\n\u001b[0;32m   1360\u001b[0m     )\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\pipelines\\base.py:1369\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1368\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1369\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1370\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\pipelines\\base.py:1269\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1268\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1269\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1270\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:383\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[0;32m    381\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m--> 383\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    384\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\generation\\utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2252\u001b[0m     )\n\u001b[0;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2256\u001b[0m         input_ids,\n\u001b[0;32m   2257\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2258\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2259\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2260\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2261\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2262\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2263\u001b[0m     )\n\u001b[0;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2275\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\generation\\utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3261\u001b[0m     outputs,\n\u001b[0;32m   3262\u001b[0m     model_kwargs,\n\u001b[0;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3264\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:836\u001b[0m, in \u001b[0;36mGemmaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    833\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    835\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 836\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    837\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    838\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    839\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    840\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    841\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    842\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    843\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    844\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    845\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    846\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    848\u001b[0m )\n\u001b[0;32m    850\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    851\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:558\u001b[0m, in \u001b[0;36mGemmaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    556\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m cache_position\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 558\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_causal_mask(\n\u001b[0;32m    559\u001b[0m     attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n\u001b[0;32m    560\u001b[0m )\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# embed positions\u001b[39;00m\n\u001b[0;32m    563\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:646\u001b[0m, in \u001b[0;36mGemmaModel._update_causal_mask\u001b[1;34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_static_cache \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[1;32m--> 646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m AttentionMaskConverter\u001b[38;5;241m.\u001b[39m_ignore_causal_mask_sdpa(\n\u001b[0;32m    647\u001b[0m         attention_mask,\n\u001b[0;32m    648\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minput_tensor,\n\u001b[0;32m    649\u001b[0m         past_key_values_length\u001b[38;5;241m=\u001b[39mpast_seen_tokens,\n\u001b[0;32m    650\u001b[0m         is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[0;32m    651\u001b[0m     ):\n\u001b[0;32m    652\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    654\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39mdtype, input_tensor\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:288\u001b[0m, in \u001b[0;36mAttentionMaskConverter._ignore_causal_mask_sdpa\u001b[1;34m(attention_mask, inputs_embeds, past_key_values_length, sliding_window, is_training)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(attention_mask\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(attention_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key_value_length \u001b[38;5;241m==\u001b[39m query_length:\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;66;03m# For query_length == 1, causal attention and bi-directional attention are the same.\u001b[39;00m\n\u001b[0;32m    291\u001b[0m         ignore_causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "restored_reviews = []\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    print(index)\n",
    "    query = row['input']\n",
    "    prompt = (r\"\"\"<bos><start_of_turn>user\n",
    "글자 수와 띄어쓰기를 유지하며 리뷰를 복원하세요:\n",
    "\n",
    "{}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\".format(query))\n",
    "\n",
    "    # 텍스트 생성\n",
    "    generated = pipe_finetuned(\n",
    "        prompt,\n",
    "        num_return_sequences=1,\n",
    "        #temperature=0.2,\n",
    "        #top_p=0.9,\n",
    "        max_new_tokens=len(query) + 2,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # 생성된 텍스트에서 출력 부분 추출\n",
    "    generated_text = generated[0]['generated_text']\n",
    "    result = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    restored_reviews.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da2dd9e9-e81c-43f2-898e-2bef6dd81e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at index 33: 신라스테이 서부산 오픈 축하드립니다!!~<end_of_turn>\n",
      "Error at index 35: 절대! 다시는 가고 싶지 않음!<end_of_turn>\n",
      "Error at index 37: 뷰가 정말 예뻤어요!!<end_of_\n",
      "Error at index 41: 뷰도 청결도 최고였습니다!!!<end_of_turn>\n",
      "Error at index 42: 직원분들이 친절해서 좋았음.<end_of_turn>\n",
      "Error at index 52: 포근하니 쾌적하구 룸 컨디션 맘에 쏘옥~<end\n",
      "Error at index 56: 뷰 맛집... 내 집이면 좋겠어요..ㅜㅜ<end_of_turn>\n",
      "Error at index 72: 좋아요. 여럿이 놀러 가면 딱이에요~<end_of_\n",
      "Error at index 73: 다시 가고 싶은 호텔 위치도 최고<end_of_turn>\n",
      "Error at index 76: 스테이크 먹었는데 맛이 최고예요. 디저트도 굿<end_of_turn>\n",
      "Error at index 80: 접근성도 좋고, 깨끗하네요.<end_of\n",
      "Error at index 89: 공용 사우나에 겸미주릴 개 많아요..<end_of_turn>\n",
      "Error at index 94: 조명이 너무 어두워요.<end_of_\n",
      "Error at index 99: 32층 다이닝룸 야경이 너무 예뻐요. 로맨틱한 저녁식사<end_of_turn>\n",
      "Error at index 102: 그다지 권해주고 싶지 않은 곳~<end_of_turn\n",
      "Error at index 116: 바다눈 좋으나 간격이 너무 좁음.<end_of_turn\n",
      "Error at index 121: 별내의 호텔 더 메이 의 구청만 내세우는 태도와 고객 불편사항에 대한 불성실, 아고다 측의 환불 규정에 문제가 많다. 당일 숙박을 위한 당일 예약 시 예상 체크인 시간 이전에는 취소가 안 된다 할지라도 숙박 내용 변경은 가능하도록 사용자 입장에서 편리성을 부여했으면 좋겠다. 예약 시에 객실 2개를 예약했다가 업무 일정 변경으로 인해 예약 후 1시간 정도 경과한 후 객실 1개에 대해 취소 요청을 예상 체크인 시간 이전에 했는데, 받아들여지지 않았다. 아고다 본사 고객센터와 호텔 프런트에도 사정을 얘기했지만, 규정만 얘기할 뿐 고객의 편의는 전혀 고려하지 않고 다른 편의사항을 요청해도 반영해 주지 않으며, 고객이 손해를 봐도 자기들은 잘못이 없다는 것이다. 아고다의 불합리한 규정과 상담원의 친절하지 않은 딱딱한 응대 방식도 문제이고, 더 메이 호텔 직원들도 고객의 입장이 아닌 아고다 규정을 핑계로 대면서... 호텔 측도 규정만 얘기할 뿐 고객의 편의는 전혀 고려하지 않고 다른 편의사항을 요청해도 반영해 주지 않으며, 고객이 손해를 봐도 자기들은 잘못이 없다는 것이다. 아고다의 불합리한 규정과 상담원의 친절하지 않은 딱딱한 응대 방식도 문제이고, 더 메이 호텔 직원들도 고객의 입장이 아닌 아고다 규정을 핑계로 대면서... 호텔 측도 규정만 얘기할 뿐 고객의 편의는 전혀 고려하지 않고 다른 편의사항을 요청해도 반영해 주지 않으며, 고객이 손해를 봐도 자기들은 잘못이 없다는 것이다. 아고다의 불합리한 규정과 상담원의 친절하지 않은 딱딱한 응대 방식도 문제이고, 더 메이 호텔 직원들도 고객의 입장이 아닌 아고다 규정을 핑계로 대면서... 호텔 측도 규정만 얘기할 뿐 고객의 편의는 전혀 고려하지 않고 다른 편의사항을 요청해도 반영해 주지\n",
      "Error at index 123: 7만원에 홍보 호객 질이눔 ㅋㅋ<end_of_turn\n",
      "Error at index 137: 위치가 좋고 깨끗해요.<end_of\n",
      "Error at index 141: 네이버에 리뷰가 없더라고요.<end_of_turn>\n",
      "Error at index 151: 다 좋은데 샤워실에 냄새가 심했네요.<end_of_turn\n",
      "Error at index 169: 광안대교 오션뷰를 즐길 수 있는 곳<end_of_turn>\n",
      "Error at index 188: 침대가 넓어서 아주 좋습니다!<end_of_turn>\n",
      "Error at index 192: 뷰가 너무 아름다워요. 푹 쉬다가 갑니다.<end_of_turn\n",
      "Error at index 202: 펜션도 좋고 사장님도 친절하시고 너무 깨끗했어요^^<end_of_turn>\n",
      "Error at index 235: 예뻐요. 창밖으로 보이는 뷰가 끝내줘요.<end_of_turn>\n",
      "Error at index 242: 아님, 고사앙 맞냐고?<end_of_\n",
      "Error at index 246: 사장님이 너무 친절하심<end_of\n",
      "Error at index 250: 지나을 가지고 다녀야겠네요.<end_of_turn\n",
      "Error at index 257: 친절하고 숙소도 깨끗해요.<end_of\n",
      "Error at index 268: 결혼식 스테이크가 맛있어요~~<end_of_\n",
      "Error at index 287: 굳굳, 아름다운 전망<end_of\n",
      "Error at index 288: 편하게 쉬었다가 아침을 맛있게 먹었어요.<end_of_turn>\n",
      "Error at index 289: 정말 힐링하고 가요. 부모님 모시고 재방문할게요.<end_of_turn>\n",
      "Error at index 298: 친구들과 즐거운 추억을 만들었어요. 최고의 숙소!<end_of_turn>\n",
      "Error at index 305: 가성비가 좋은 숙소입니다. 강추<end_of_turn>\n",
      "Error at index 308: 시설 괜찮은데 서비스 신경 좀 써주세요.<end_of_turn>\n",
      "Error at index 316: 깔끔해요. 조식도 가성비가 굿<end_of\n",
      "Error at index 321: 친절하시고 방이 커서 좋았어요.<end_of_turn\n",
      "Error at index 329: 풍광이 너무 아름답네요~<end_of\n",
      "Error at index 340: 압도적인 뷰, 준수한 서비스<end_of_turn>\n",
      "Error at index 364: 테라스 욕조 존음  직원분 왕친절<end_of_\n",
      "Error at index 366: 옷걸이가 하나도 없어요, 욕실에도, 슬리퍼도<end_of_turn>\n",
      "Error at index 401: 되게 깔끔하고 사장님도 인심이 좋으세요.<end_of_turn>\n",
      "Error at index 412: 여기가 황당 맛집이라는 곳인가?<end_of_turn\n",
      "Error at index 425: 일단 비즈니스급 호텔이지 별 4개는 과한 것 같습니다. 침구류와 객실이 개장 안 되어 묵은 먼지가 없는 점은 괜찮지만, 완강히 안내가 눈에 확 들어오지 않는 것, 창문 프레임 고정 못이 이만큼 녹슨 건지 크리스 영향인지, 녹이 나 보이는 저는 객실 내 바로 눈에 띄는 마이너스 요소였고, 검은 암막이 답답하게 만드네요. 벽 색상과 맞춰 어두운 블루 톤으로 해도 괜찮았을 텐데. 화장실은 쾌적하고, 다행히 제 객실은 역류나 냄새가 없었어요. 다만, 고층으로 요청했는데, 13층 창밖 풍경은 남산타워 보이는 것과 영락교회 보이는 것 외엔 별로였습니다. 층에서 1번대 방향은 다 갔을 테니, 참조하세요. 화장품 어메니티는 핸드크림만 있으니 꼭 뭔가 챙겨와야 하고요. 핸드타올이 양 노후 된 수건, 원래 서울에서는 비데용 타올 아닌가요? 한국은 다르게 두는 건가요? 여기에 한국인만 묵는 것도 아니고, 터군다나 외국계 호텔이 그 점을 모를 리도 없는데... 한국을 무시하는 첫사인가요? 로고도 좋지만, 용도를 창량리 휘트 층 수 있는 이름을 넣어두는 게 오해를 막을 것 같습니다. 수건 걸이가 화장실에 사워실 문고리 외에 없다는 안타까움에 별 하나 깎고요. 조식은 투숙객에게 12천 원대에 파는데, 네... 그거면 차라리 전날 롯데에서 파는 연어 샐러드 할인을 사두었다 먹겠어요. 만지 투숙객은 맛도 모르는 사람들이죠. 지널 & 품목 구성인데요, 아침 빵 종류는 기본도 안 되어 있어요. 모닝빵은 어제 쓴 거 냉장고에 넣었다가 다시 진 모양새 혹은 부여서 스팀했는지, 서로 부딪혀서 겹칠 뻔해 있어서 뽑기엔 후했고요. 베이글은 다 마르게 얼심히 반으로 갈라놨더라구요. 기계도 많은데, 손님 보고 알아서 차라서 구울라 해요. 코로나 시대에, 일하는 사람들의 기침이 무엇인지 침이 튀었는지 알 수도 없이 모두 나란히 펼쳐 놓은 것은 무슨 의도인지, 당황스러웠습니다. 또, 아시아식 음식은 하나도 맛이 없었어요. 만두는 맛이 없어서 아무도 안 가져가고, 가져가는 건 새우 들어간 것만 다들 집어서, 두어 명 집어가면 ㅋㅋㅋ 딸감 규수는 가는 국수에 간을 너무 세게 해서... 먹을 수가 없어요. 차오 차... 두부 요리는 정체가? 차라리 두 개를 버리고 제대로 달걀 넣어 볶은 쾌유타이아오를 말려하든가, 쌀국수가 있으니 그냥 다 버리고 동남아 튀긴 엄묵을 차라리 고수, 쌀어 튀김이라 먹으라 내놓든가. 대 실망이요. 한국 호텔인데 떡도 없었고, 시계 정도는 충분히 수리용, 오마자 첼리 같은 걸 내어도 되는데... 이건 별 세 개 이탤리 관광 호텔보다 못했습니다. 그냥 저냥 딱 보이게 서양식 메뉴는 먹을 만 했는데, 그나마 쌀국수와 에그 스크램블 때문에 미국식 하케이크가 없어요. 그냥 어메한 서양식 무근본 조식입니다. 또, 아시아식 음식은 한나도 맛이 없었어요. 만두는 맛이 없어서 아무도 안 가져가고, 가져가는 건 새우 들어간 것만 다들 집어서, 두어 명 집어가면 ㅋㅋㅋ 딸감 규수는 가는 국수에 간을 너무 세게 해서... 먹을 수가 없어요. 차오 차... 두부 요리는 정체가? 차라리 두 개를 버리고 제대로 달걀 넣어 볶은 쾌유타이아오를 말려하든가, 쌀국수가 있으니 그냥 다 버리고 동남아 튀긴 엄묵을 차라리 고수, 쌀어 튀김이라 먹으라 내놓든가. 대 실망이요. 한국 호텔인데 떡도 없었고, 시계 정도는 충분히 수리용, 오마자 첼리 같은 걸 내어도 되는데... 이건 별 세 개 이탤리 관광 호텔보다 못했습니다. 그냥 저냥 딱 보이게 서양식 메뉴는 먹을 만 했는데, 그나마 쌀국수와 에그 스크램블 때문에 미국식 하케이크가 없어요. 그냥 어메한 서양식 무근본 조식입니다. 또, 아시아식 음식은 한나도 맛이 없었어요. 만두는 맛이 없어서 아무도 안 가져가고, 가져가는 건 새우 들어간 것만 다들 집어서, 두어 명 집어가면 ㅋㅋㅋ 딸감 규수는 가는 국수에 간을 너무 세게 해서... 먹을 수가 없어요. 차오 차... 두부 요리는 정체가? 차라리 두 개를 버리고 제대로 달걀 넣어 볶은 쾌유타이아오를 말려하든가, 쌀국수가 있으니 그냥 다 버리고 동남아 튀긴 엄묵을 차라리 고수, 쌀어 튀김이라 먹으라 내놓든가. 대 실망이요. 한국 호텔인데 떡도 없었고, 시계 정도는 충분히 수리용, 오마자 첼리 같은 걸 내어도 되는데... 이건 별 세 개 이탤리 관광 호텔보다 못했습니다. 그냥 저냥 딱 보\n",
      "Error at index 431: 좋아요, 담에도 숙시하고 싶어요.<end_of_turn>\n",
      "Error at index 444: 주차가 진짜 불편, 그거 빼고는 좋음.<end_of_turn>\n",
      "Error at index 453: 호텔에 심심한 에토를 포함합니다.<end_of_turn>\n",
      "Error at index 455: 사장님이 참 친절하시고 힐링하기에는 딱이에요^^<end_of_turn>\n",
      "Error at index 466: 사장님이 불친절해 보여요.<end_of_\n",
      "Error at index 468: 좋아요, 음식도 너무너무 맛있어요.<end_of_\n",
      "Error at index 475: 그냥 최고입니다! 굿굿<end_of_\n",
      "Error at index 482: 귀찮은 듯 불친절하네요.<end_of\n",
      "Error at index 485: 직원분들이 너무 친절하시고 음식도 너무 맛있었어요!!<end_of_turn>\n",
      "Error at index 493: 우풍이 심해서 추워요ㅠ<end_of_\n",
      "Error at index 495: 식당에 맛집이 많아<end_\n",
      "Error at index 497: 최악. 아웃티 끄는 사장님이 막 화내심..ㅜㅜ<end_of_turn\n",
      "Error at index 501: 많이 지저분하네요… 이불에서도 냄새 나요.<end_of_turn>\n",
      "Error at index 517: 깨끗하고 조금만 오션뷰도 있어서 좋았어요~<end_of_turn>\n",
      "Error at index 523: 테라스가 있어서 편했어요.<end_of_turn\n",
      "Error at index 534: 퇴근 시간 미리는 급하게 쫌 가서 잘 가요.<end_of_turn>\n",
      "Error at index 565: 침대만 조금만 더 좋다면 5점입니다.<end_of_turn>\n",
      "Error at index 593: 학회 때문에 다녀왔어요. 좋아요<end_of_turn>\n",
      "Error at index 609: 합리적인 가격, 만족스러운 청결도<end_of_turn\n",
      "Error at index 615: 수영장이 있어서 좋네요.<end_of_turn\n",
      "Error at index 623: 무인 시스템이라 편하게 지내다 왔네요.<end_of_turn>\n",
      "Error at index 631: 호텔 직원분들 고생 많으십니다.<end_of_turn>\n",
      "Error at index 658: 방 컨디션도 좋고 편안하게 잘 쉬다 갑니다<end_of_turn>\n",
      "Error at index 664: 깔끔하고 친절하고 ㅎㅎ<end_of_\n",
      "Error at index 678: 서촌효수, 롯데타워, 롯데월드, 꽃든 래지.. 굿 숙소 엘리베이터가 좀 헷갈림.<end_of_turn\n",
      "Error at index 679: 호텔 컨디션이 너무 좋아요. 굿굿<end_of_\n",
      "Error at index 687: 직원이 방을 깨끗하게 치우지 않았다.<end_of_turn>\n",
      "Error at index 692: 당일 취소라니 알량뜨겠습니다.<end_of_turn>\n",
      "Error at index 698: 시설도 괜찮고 넘넘 친절하셨어요~ 무엇보다 침대가 푹신푹신해서 잘 자다 왔네요~ㅎㅎ<end_of_turn>\n",
      "Error at index 700: 돈이 맥살해서 그렇지 다 좋다.<end_of_turn>\n",
      "Error at index 718: 부르지요 시티에 있는 숙박 호텔이다.<end_of_turn>\n",
      "Error at index 722: 오랜만에 호캉스 + 해외 일정 후 쉼을 위해 일부러 부담되는 가격임에도 코너 오션뷰를 선택했음. 첫날 들어가자마자 컵 위생 상태가 엉망이라 ‘이게 먹지..’ 싶었음. 그래도 뭐, 사람이 하는 일이니 그런러니 했는데, 저녁에 샤워하려 샤워실 들어가니 샤워기와 연결된 물건 올려둘 수 있는 선반 같은 곳에 먼지와 물때 같은 것이 보였음. 올려뒀던 물건 다 치우고 물로 한 번 씻어낸 뒤에 사용해야 했음. 이미 물을 뿌렸고 샤워하려 벗은 상태로 들어가서 사진은 못 찍음. 다음날 외출하면서 청소 상태가 안 좋았다고 하면 오늘 청소할 때 신경 써달라고 일부러 조용히 말했는데, 프론트에 있던 직원분은 '아, 네.. 전달하겠습니다.' 하고 끗. 불편하게 해드려 죄송하려 최송하다고 마린나 신경 쓰겠다는 그 어떤 인사도 없이 그저 내가 한 말을 전달하겠다가가 끝.. 오청한 걸 해준다 하니 그저 감사합니다 해야 하는건가. 그리고 조식은 나쁘지 않았으나, 손님이 남긴 테이블 매트는 절대 다시 갈리지 않음. 테이블 매트 갈린 곳에 좌차서 앉던가 아니면 원하는 자리가 아니면 테이블 매트 없는 채로 앉아야 함. 이틀 내내 그런 모습이었음. 그래서 두 번째 나른 사진 찍음. 식당 이용 제한 시간 15분 전후로 없고, 음식이 언제 나올지 몰라 앉아 있을 뿐인데 2일 차에는 샐러드 소스 3가지 모퉁 20분 전에는 비어 있었음. 크리고 에어컨 바람 때문에 공기 중에 날리는데 보인 몸을 잊은 아줌머니 직원 두 분이 큰 소리로 대화하겠다고 쇼도줴인지 뭔지를 찍찍 불리면 테이블을 딱. 에어컨 바람 때문에 공기 중에 날리는데 보인 몸을 잊은 아줌머니 직원 두 분이 큰 소리로 대화하겠다고 쇼도줴인지 뭔지를 찍찍 불리면 테이블을 딱. 1층 로비에서는 엘리베이터 기다리는데 청소 업무를 하시는 걸로 보이는 유니폼을 입으신 아줌머니 직원 두 분이 큰 소리로 대화하심. 그것도 엘리베이터 앞에 서 있는 나와 일행을 가운데 두고 서서 하는 것도 아니고, 심지어 테이블 밑으로 엉덩이를 빼고 앉는 것도 아니고 그냥 곁에서 찍찍 불리고 닦고 감. 1층 로비에서는 엘리베이터 기다리는데 청소 업무를 하시는 걸로 보이는 유니폼을 입으신 아줌머니 직원 두 분이 큰 소리로 대화하심. 그것도 엘리베이터 앞에 서 있는 나와 일행을 가운데 두고 서서 하는 것도 아니고, 심지어 테이블 밑으로 엉덩이를 빼고 앉는 것도 아니고 그냥 곁에서 찍찍 불리고 닦고 감. 1층 로비에서는 엘리베이터 기다리는데 청소 업무를 하시는 걸로 보이는 유니폼을 입으신 아줌머니 직원 두 분이 큰 소리로 대화하심. 그것도 엘리베이터 앞에 서 있는 나와 일행을 가운데 두고 서서 하는 것도 아니고, 심지어 테이블 밑으로 엉덩이를 빼고 앉는 것도 아니고 그냥 곁에서 찍찍 불리고 닦고 감. 1층 로비에서는 엘리베이터 기다리는데 청소 업무를 하시는 걸로 보이는 유니폼을 입으신 아줌머니 직원 두 분이 큰 소리로 대화하심. 그것도 엘리베이터 앞에 서 있는 나와 일행을 가운데 두고 서서 하는 것도 아니고, 심지어 테이블 밑으로 엉덩이를 빼고 앉는 것도 아니고 그냥 곁에서 찍찍 불리고 닦고 감. 1층 로비에서는 엘리베이터 기다리는데 청소 업무를 하시는 걸로 보이는 유니폼을 입으신 아줌머니 직원 두 분이 큰 소리로 대화하심. 그것도 엘리베이터 앞에 서 있는 나와 일행을 가운데 두고 서서 하는 것도 아니고, 심지어 테이블 밑으로 엉덩이를 빼고 앉는 것도 아니고 그냥 곁에서 찍찍 불리고 닦고 감. 1층 로비에서는 엘리베이터 기다리는데 청소 업무를 하시는 걸로 보이는 유니폼을 입으신 아줌머니 직원 두 분이 큰 소리로 대화하심. 그것도 엘리베이터 앞에 서 있는 나와 일행을 가운데 두고 서서 하는 것도 아니고, 심지어 테이블 밑으로 엉덩이를 빼고 앉는 것도 아니고 그냥 곁에서 찍찍 불리고 닦고 감. 1층 로비에서는 엘리베이터 기다리는데 청소 업무를 하시는 걸로 보이는 유니폼을 입으신 아줌머니 직원 두 분이 큰 소리로 대화하심. 그것도 엘리베이터 앞에 서 있는 나와 일행을 가운데 두고 서서 하는 것도 아니고, 심지어 테이블 밑으로 엉덩이를 빼고 앉는 것도 아니고 그냥 곁에서 찍찍 불리고 닦고 감. 1층 로비에서는 엘리베이터 기다리는데 청소 업무를 하시는 걸로 보이는 유니폼을 입으신 아줌머니 직원 두 분이 큰 소리로 대화하심. 그것도 엘리베이터 앞에 서 있는 나와 일행을 가운데 두고 서서 하는 것도 아니고, 심지어 테이블 밑으로 엉덩이를 빼고 앉는 것도 아니고 그냥 곁에서 찍찍 불리고 닦고 감. 1층 로비에서는 엘리베이터 기다리는데 청소 업무를 하시는 걸로 보이는 유니폼을 입으신 아줌머니 직원 두 분이 큰 소리로 대화하심. 그것도 엘리베이터 앞에 서 있는 나와 일행을 가운데 두고 서서 하는 것도 아니고, 심지어 테이블 밑으로 엉덩이를 빼고 앉는 것도 아니고 그냥 곁에서 찍찍 불리고 닦고 감. 1층 로비에서는 엘리베이터 기다리는데 청소 업무를 하시는 걸로 보이는 유니폼을 입으신 아줌머니 직원 두 분이 큰 소리로 대화하심. 그것도 엘리베이터 앞에 서 있는 나와 일행을 가운데 두고 서서 하는 것도 아니고, 심지어 테이블 밑으로 엉덩이를 빼고 앉는 것도 아니고 그냥 곁에서 찍찍 불리고 닦고 감. 1층 로비에서는 엘리베이터 기다리는데 청소 업무를 하시는 걸로 보이는 유니폼을 입으신 아줌머니 직원 두 분이 큰 소리로 대화하심. 그것도 엘리베이터 앞에 서 있는 나와 일행을 가운데 두고 서서 하는 것도 아니고, 심지어 테이블 밑으로 엉덩이를 빼고 앉는 것도 아니고 그냥 곁에서 찍찍 불리고 닦고 감. 1층 로비에서는 엘리베이터 기다리는데 청소 업무를 하시는 걸로 보이는 유니폼을 입으신 아줌머니 직원 두 분이 큰 소리로 대화하심. 그것도 엘리베이터 앞에 서 있는 나\n",
      "Error at index 726: 객실이 깔끔하고 시설이 좋으며 조식이 맛있었어요!<end_of_turn>\n",
      "Error at index 729: 여기는 안 가야겠다. 저장해야겠다.<end_of_turn>\n",
      "Error at index 734: 출장 가면 여기에서만 자요.<end_of_turn>\n",
      "Error at index 754: 깨끗하고 다 좋아요~~~<end_of_turn>\n",
      "Error at index 773: 역가 취소 맛집입니다?<end_of_turn\n",
      "Error at index 775: 가족 호텔, 연회장, 2인실, 세미나실, 패밀리룸, 워크숍<end_of_turn\n",
      "Error at index 786: 에어컨이 좀 아쉬운 성능이지만 그래도 컨디 만함.<end_of_turn>\n",
      "Error at index 790: 오션뷰 진짜 대박이에요ㅋㅋㅋ<end_of_\n",
      "Error at index 797: 뷰 맛집 낮에는 오션뷰, 밤에는 광안대교 뷰<end_of_turn\n",
      "Error at index 804: 디톡스 맛집인 딱 좋은 곳<end_of\n",
      "Error at index 808: 여차 관리인 불친절하고 까칠함..<end_of_turn>\n",
      "Error at index 822: 오션뷰가 밝고니도 있썽 짱임. 시내 쪽은 가이라도 엄청 싸면 좋을 듯하다.<end_of_turn>\n",
      "Error at index 829: 뷰도 좋고 깨끗해요! 재방문할게요!<end_of_\n",
      "Error at index 830: 가성비와 접근성이 좋은 호텔<end_of_turn\n",
      "Error at index 848: 해운대에서 한 달 살기 성공<end_of_turn>\n",
      "Error at index 852: 직원분이 친절하고 호텔이 깨끗해요.<end_of_turn\n",
      "Error at index 866: 해운대 해수욕장 뷰가 미쳤어요.<end_of_\n",
      "Error at index 876: 식물원 스카이야드 @ 4층<end_of\n",
      "Error at index 884: 13층 루프탑에서 연말 파티를 했었습니다. 루프탑의 이용 시간은 오후 2시부터 새벽 1시까지이며, 파티룸이라는 공간에 알맞게 잘 조성되어 있었습니다. 넉넉한 공간과 테이블, 의자, 소파도 있었습니다. 객실과는 달리 신발을 신고 돌아다녀도 무방해서 더 편리했습니다. 문을 열고 들어가자마자 보이는 전망이 인상적입니다. 3면이 전부 통유리창으로 되어 있어 푸악, 낙산, 동대문 및 서울 일대의 전경이 인상 깊었습니다. 이 호텔 위치의 13층에서 이렇게까지 전망이 좋을 줄은 몰랐는데, 막상 올라와 보니 너무 좋았습니다. 탁 트인 느낌 덕분에 더 쾌적하게 느껴졌습니다. 루프탑이라 날씨가 안 좋을 때가 걱정될 수도 있겠으나, 그럴 걱정이 필요 없을 만큼 아주 좋습니다. 루프탑이라 함면 보통 실내 공간보다 바깥의 공간이 넓지만, 이곳은 둥 공간 전부 잘 되어 있습니다. 실내 공간만 서도 충분한 크기입니다. 냉난방 시설 모두 훌륭했고 창호도 튼튼하게 잘 되어 있어 영하가 넘는 날씨에도 불구하고 따뜻하게 잘 보낼 수 있었습니다. 바베큐도 아주 크게 되어 있으며, 바베큐와 객실 사이에는 폴딩 도어가 설치되어 있어 날이 좋을 때에는 폴딩 도어를 아예 열어두고 사용할 수도 있게 되어 있습니다. 바티룸에 펜놀을 쓸 수 없는 음향도 잘 되어 있는 편입니다. 2채넘 스피커와 빡어트 믹서가 설치되어 있습니다. PC나 핸드폰의 AUX 케이블을 연결하여 사용할 수 있게 되어 있습니다. 또한 프로젝터도 사용할 수 있었지만 따로 사용하지는 않았습니다. 벽면이 전부 흰색이기 때문에 별도의 스크린 설치 없이도 쾌적하게 사용할 수 있겠습니다. 음향에서야 블루투스 리시버가 있다면 조금 더 편리하게 이용할 수 있겠으나, 아무래도 프로젝터와 같이 사용하기에는 디레이가 있는 블루투스보다는 AUX 케이블이 최선일 것입니다. 테문에 이처럼 설치해 놓지 않았나 싶습니다. 애매한 블루투스 홈바를 갖다 두는 것보다, 차라리 이렇게 안정적인 구성의 본인은 훨씬 좋습니다. 천장 조명도 총 6줄로 셀프적으로 조절 가능하였습니다. 처음에 음식을 먹을 때는 밝게 먹다가, 술 마시고 시간이 지나면서 조금 어둡게 줄기고 싶을 때는 창문 쪽만 켜서 간접 조명으로 사용했는데, 분위기 조절하는 데 아주 좋았습니다. 단순해 보이지만 놓치기 쉬운 이런 디테일이 잘 되어 있어서 좋았습니다. 바티룸답게 콘센트도 곳곳에 넉넉히 있어서 콘센트가 굳이 필요 없습니다. 시설에 감탄하다 보니 직원분들도 친절하시고, 지금이야 이렇게 안정적인 분위기 대신 낡은 분위기 때문에 걱정할 필요는 없을 것 같습니다. 오히려 복잡한 해황나 동대문 한복판보다 훨씬 좋았습니다. 인근에 스벅마트나 편의점도 곳곳에 있고, 맛있는 집들이 많아서 편리했습니다. 루프탑뿐만 아니라 호텔 전반적으로 신축이다 보니 인근의 낙후된 분위기 때문에 걱정할 필요는 없을 것 같습니다. 오히려 복잡한 해황나 동대문 한복판보다 훨씬 좋았습니다. 인근에 스벅마트나 편의점도 곳곳에 있고, 맛있는 집들이 많아서 편리했습니다. 루프탑뿐만 아니라 호텔 전반적으로 신축이다 보니 인근의 낙후된 분위기 때문에 걱정할 필요는 없을 것 같습니다. 오히려 복잡한 해황나 동대문 한복판보다 훨씬 좋았습니다. 인근에 스벅마트나 편의점도 곳곳에 있고, 맛있는 집들이 많아서 편리했습니다. 루프탑뿐만 아니라 호텔 전반적으로 신축이다 보니 인근의 낙후된 분위기 때문에 걱정할 필요는 없을 것 같습니다. 오히려 복잡한 해황나 동대문 한복판보다 훨씬 좋았습니다. 인근에 스벅마트나 편의점도 곳곳에 있고, 맛있는 집들이 많아서 편리했습니다. 루프탑뿐만 아니라 호텔 전반적으로 신축이다 보니 인근의 낙후된 분위기 때문에 걱정할 필요는 없을 것 같습니다. 오히려 복잡한 해황나 동대문 한복판보다 훨씬 좋았습니다. 인근에 스벅마트나 편의점도 곳곳에 있고, 맛있는 집들이 많아서 편리했습니다. 루프탑뿐만 아니라 호텔 전반적으로 신축이다 보니 인근의 낙후된 분위기 때문에 걱정할 필요는 없을 것 같습니다. 오히려 복잡한 해황나 동대문 한복판보다 훨씬 좋았습니다. 인근에 스벅마트나 편의점도 곳곳에 있고, 맛있는 집들이 많아서 편리했습니다. 루프탑뿐만 아니라 호텔 전반적으로 신축이다 보니 인근의 낙후된 분위기 때문에 걱정할 필요는 없을 것 같습니다. 오히려 복잡한 해황나 동대문 한복판보다 훨씬 좋았습니다. 인근에 스벅마트나 편의점도 곳곳에 있고, 맛있는 집들이 많아서 편리했습니다. 루프탑뿐만 아니라 호텔 전반적으로 신축이다 보니 인근의 낙후된 분위기 때문에 걱정할 필요는 없을 것 같습니다. 오히려 복잡한 해황나 동대문 한복판보다 훨씬 좋았습니다. 인근에 스벅마트나 편의점도 곳곳에 있고, 맛있는 집들이 많아서 편리했습니다. 루프탑뿐만 아니라 호텔 전반적으로 신축이다 보니 인근의 낙후된 분위기 때문에 걱정할 필요는 없을 것 같습니다. 오히려 복잡한 해황나 동대문 한복판보다 훨씬 좋았습니다. 인근에 스벅마트나 편의점도 곳곳에 있고, 맛있는 집들이 많아서 편리했습니다. 루프탑뿐만 아니라 호텔 전반적으로 신축이다 보니 인근의 낙후\n",
      "Error at index 885: 잊지 못할 매너 하우스 개 굳<end_of_turn>\n",
      "Error at index 891: 나빠빠리? 그런 거 나라다님<end_of_\n",
      "Error at index 897: 버나조식 맛있어요~~<end_\n",
      "Error at index 916: 친절하시고 따뜻하며 깨끗합니다.<end_of_turn>\n",
      "Error at index 918: 친절하고 분위기가 좋네요<end_of_\n",
      "Error at index 932: 위치: 베리베리 굿 시설은....<end_of_turn>\n",
      "Error at index 934: 뷰가 너무 좋고 깔끔해서 최고였습니당!!!<end_of_turn>\n",
      "Error at index 936: 리모델링 해서 깨끗합니다.<end_of_turn\n",
      "Error at index 941: 공사 중이어서 시끄럽고, 난방이 고장나서 추웠습니다 ㅠ<end_of_turn>\n",
      "Error at index 960: 주차할 때 좀 불편해 고, 뭐 그거 빼곤 다 괜찮았어요.<end_of_turn>\n",
      "Error at index 980: 가성비 좋은 곳 ^^<end_of_turn>\n",
      "Error at index 1001: 낡고 좁다. 비스타가 낫다.<end_of_\n",
      "Error at index 1009: 바다 뷰 엄청 예뻐요!!<end_of_\n",
      "Error at index 1019: 올디스 벗 굿티스<end\n",
      "Error at index 1024: 가성비가 갑! 깨끗해요. 친절해요. 8시에 왔는데 매점이 닫았어요. ㅠㅠ<end_of_\n",
      "Error at index 1031: 작고 고급스러운 부티크형 호텔<end_of_\n",
      "Error at index 1035: 1층에서부터 인싸해 주고 선비수가 듦림ㅋㅋ 굳<end_of_turn>\n",
      "Error at index 1037: 오픈 축하드립니다!!<end_of_\n",
      "Error at index 1049: 겸손식 뷔페 맛있 특히 아이스크림이 베이커리 느낌<end_of_turn>\n",
      "Error at index 1052: 완벽하고 가격도 쌉니다.<end_of_turn\n",
      "Error at index 1059: 객실은 괜찮으나 방음이 최악입니다.<end_of_turn>\n",
      "Error at index 1060: 믿고 거르기 위한 저장<end_of_turn>\n",
      "Error at index 1065: 낮뷰, 밤뷰 다 예뻐요!<end_of_\n",
      "Error at index 1073: 평창은 스위스 알프스 같은 곳이에요.<end_of_turn>\n",
      "Error at index 1075: 조식, 수영장, 룸이 좋아요.<end_of_turn\n",
      "Error at index 1084: 깨끗하고 친절하고 좋아요.<end_of_turn>\n",
      "Error at index 1089: 귀여워요 뽀겟모부<end\n",
      "Error at index 1090: 오래된 곳이에요. 감안하세요.<end_of_turn>\n",
      "Error at index 1112: 모조리 느끼고 갑니다~<end_of\n",
      "Error at index 1126: 시설과 전망이 매우 훌륭하네요. 밤에 꼭 숙박하고 싶네요.<end_of_turn>\n",
      "Error at index 1130: 방이 깨끗하고 넓고 좋아요. 사장님도 친절해요.<end_of_turn>\n",
      "Error at index 1140: 깨끗하고 접근성이 좋다.<end_of_turn\n",
      "Error at index 1153: 6시 체크인 하기 전 카운터에 아무도 없어서 로비에 앉아 음료를 마시며 기다렸어요. 로비 시설도 좋고 음료도 추가로 먹을 수 있어 그 점은 좋았어요. 그런데 10분 전 미리 체크인 가능할까 카운터에 직원이 요청해서 예약한 사이트와 성함 말씀드리고 객실 키를 받았습니다. 그런데 도착한 방은 제가 예약한 비즈니스룸이 아니었습니다. 그래서 프런트에 전화해서 이야기했더니 롱비로 체크인 하기 전 까운터에 아무도 없어서 로비에 앉아 음료를 마시며 기다렸어요. 로비 시설도 좋고 음료도 추가로 먹을 수 있어 그 점은 좋았어요. 그런데 구분이 좀 친절한 인상은 아니었고 무뚝뚝한 스타일이라 서비스치에는 잘 안 맞는다 생각하고 객실 키를 받아서 올라갔습니다. 그런데 도착한 방은 제가 예약한 비즈니스룸이 아니었습니다. 그래서 프런트에 전화해서 이야기했더니 롱비로 체크인 하기 전 카운터에 아무도 없어서 로비에 앉아 음료를 마시며 기다렸어요. 로비 시설도 좋고 음료도 추가로 먹을 수 있어 그 점은 좋았어요. 그런데 구분이 좀 친절한 인상은 아니었고 무뚝뚝한 스타일이라 서비스치에는 잘 안 맞는다 생각하고 객실 키를 받아서 올라갔습니다. 그런데 도착한 방은 제가 예약한 비즈니스룸이 아니었습니다. 그래서 프런트에 전화해서 이야기했더니 롱비로 체크인 하기 전 카운터에 아무도 없어서 로비에 앉아 음료를 마시며 기다렸어요. 로비 시설도 좋고 음료도 추가로 먹을 수 있어 그 점은 좋았어요. 그런데 구분이 좀 친절한 인상은 아니었고 무뚝뚝한 스타일이라 서비스치에는 잘 안 맞는다 생각하고 객실 키를 받아서 올라갔습니다. 그런데 도착한 방은 제가 예약한 비즈니스룸이 아니었습니다. 그래서 프런트에 전화해서 이야기했더니 롱비로 체크인 하기 전 카운터에 아무도 없어서 로비에 앉아 음료를 마시며 기다렸어요. 로비 시설도 좋고 음료도 추가로 먹을 수 있어 그 점은 좋았어요. 그런데 구분이 좀 친절한 인상은 아니었고 무뚝뚝한 스타일이라 서비스치에는 잘 안 맞는다 생각하고 객실 키를 받아서 올라갔습니다. 그런데 도착한 방은 제가 예약한 비즈니스룸이 아니었습니다. 그래서 프런트에 전화해서 이야기했더니 롱비로 체크인 하기 전 카운터에 아무도 없어서 로비에 앉아 음료를 마시며 기다렸어요. 로비 시설도 좋고 음료도 추가로 먹을 수 있어 그 점은 좋았어요. 그런데 구분이 좀 친절한 인상은 아니었고 무뚝뚝한 스타일이라 서비스치에는 잘 안 맞는다 생각하고 객실 키를 받아서 올라갔습니다. 그런데 도착한 방은 제가 예약한 비즈니스룸이 아니었습니다. 그래서 프런트에 전화해서 이야기했더니 롱비로 체크인 하기 전 카운터에 아무도 없어서 로비에 앉아 음료를 마시며 기다렸어요. 로비 시설도 좋고 음료도 추가로 먹을 수 있어 그 점은 좋았어요. 그런데 구분이 좀 친절한 인상은 아니었고 무뚝뚝한 스타일이라 서비스치에는 잘 안 맞는다 생각하고 객실 키를 받아서 올라갔습니다. 그런데 도착한 방은 제가 예약한 비즈니스룸이 아니었습니다. 그래서 프런트에 전화해서 이야기했더니 롱비로 체크인 하기 전 카운터에 아무도 없어서 로비에 앉아 음료를 마시며 기다렸어요. 로비 시설도 좋고 음료도 추가로 먹을 수 있어 그 점은 좋았어요. 그런데 구분이 좀 친절한 인상은 아니었고 무뚝뚝한 스타일이라 서비스치에는 잘 안 맞는다 생각하고 객실 키를 받아서 올라갔습니다. 그런데 도착한 방은 제가 예약한 비즈니스룸이 아니었습니다. 그래서 프런트에 전화해서 이야기했더니 롱비로 체크인 하기 전 카운터에 아무도 없어서 로비에 앉아 음료를 마시며 기다렸어요. 로비 시설도 좋고 음료도 추가로 먹을 수 있어 그 점은 좋았어요. 그런데 구분이 좀 친절한 인상은 아니었고 무뚝뚝한 스타일이라 서비스치에는 잘 안 맞는다 생각하고 객실 키를 받아서 올라갔습니다. 그런데 도착한 방은 제가 예약한 비즈니스룸이 아니었습니다. 그래서 프런트에 전화해서 이야기했더니 롱비로 체크인 하기 전 카운터에 아무도 없어서 로비에 앉아 음료를 마시며 기다렸어요. 로비 시설도 좋고 음료도 추가로 먹을 수 있어 그 점은 좋았어요. 그런데 구분이 좀 친절한 인상은 아니었\n",
      "Error at index 1155: 예약 응대가 세상 불친절한 싸우로. 비추천.<end_of_turn>\n",
      "Error at index 1158: 나만의 공간이 깊은 곳<end_of\n",
      "Error at index 1164: 깨끗하고 쾌적한 시설, 친절한 안내<end_of_turn\n",
      "Error at index 1170: 인근 물가가 비싸네요.<end_of_\n",
      "Error at index 1171: 너무너무 좋았어요. 또 가고 싶어요.<end_of_turn>\n",
      "Error at index 1177: 조식이 아주 맛있어요.<end_of\n",
      "Error at index 1186: 닭장에 닭 새끼들이 밤새 시끄러움...<end_of_turn>\n",
      "Error at index 1191: 객실에 비데만 있으면 최고일 듯요.<end_of_turn>\n",
      "Error at index 1192: 그냥... 잘 모르겠음<end_of_turn\n",
      "Error at index 1196: 방음이 진짜 최악이네요.....<end_of_turn>\n",
      "Error at index 1205: 직원분이 친절하시고 깨끗합니다. 잘 쉬고 왔네요.<end_of_turn>\n",
      "Error at index 1211: 악덕 기업인 오상<end_of\n",
      "Error at index 1248: 곧 하해짐 호텔이군요.<end_of_\n",
      "Error at index 1253: 우연히 가봤는데, 나중에 캠핑하러 가야겠어요.<end_of_turn>\n",
      "Error at index 1258: 아늑하고 깨끗합니다. 조식 샌드위치랑 커피도 맛있어요.<end_of_turn>\n",
      "Error at index 1262: 객실도 좋고 직원분들이 너무 친절해요.<end_of_turn>\n",
      "Error at index 1264: 등산충, 프룬의 성지 모텔<end_of_\n",
      "Error at index 1271: 직원분들이 친절하시고 객실이 깔끔해요!<end_of_\n",
      "Error at index 1283: 친절하시고 좋아요! 비즈니스 호텔인 듯?<end_of_turn>\n",
      "Error at index 1296: 춘수이등급 침실에 높은 전망, 고급스러운 로비 등<end_of_turn>\n",
      "Error at index 1333: 이뻐요. 주차장이 아쉽긴 한데ㅜ<end_of_\n",
      "Error at index 1340: 이불과 요에서 결레 신내 엄청난.<end_of_turn>\n",
      "Error at index 1350: 하수구 냄새 좀 없애주세요. 제발..<end_of_turn>\n",
      "Error at index 1352: 방 깨끗함, 직원분 친절함, 예쁨, 밥 맛있음<end_of_\n",
      "Error at index 1355: 제주 바닷처럼 맑네. 그늘은 없네.<end_of_turn>\n",
      "Error at index 1359: 비프가 약하거나 아기를 데리고 수영장이나 사우나에 가실 분들은 그냥 가지 않는 게 좋으실 겁니다. 일단 저는 이 호텔 이용하기 바로 전날 힐튼 경주에서도 사우나를 이용했지만, 아무런 문제가 없었습니다. 밤 9시-10시에 사우나 냉탕에 들어갔다 나왔는데 배꼽 아래로 물에 담근 부분이 새빨개져서 자카워서 발개졌구나 데쓰로직 않게 생각하고 방으로 올라왔습니다. 그런데 점점 가려고 다가어서 봤더니 비프가 새빨갛게 다 부풀어 있었습니다. 배, 어깨, 무릎, 손이 빨갛게 부풀어 올라서 일단 바디로션을 바르고 자고, 다음날 체크아웃할 때 얘기 드렸습니다. 그 이후로 몇칠 동안 통화를 했고 결론적으로 호텔 측에서 보혐사와 얘기를 나눴고, 보험사 측에선 인과관계를 증명하기 어렵다고 했다며 어쨌든 호텔을 이용하고 문제가 생겼으니 본인들 호텔 수영장 이용권을 주겠다고 합니다. 그런데 보혐사와 저랑 무슨 상관인가요? 인과관계 어쩌구는 호텔과 보험사 사이의 일이 아닌가요? 또 제가 써야 하는 거 알고 있고 호텔 사우나를 이용하여 비프에 문제가 생겼다는 걸 알면서도 수영장을 오라곤 하는 게 참 오만하고 재수가 없는 대처이고, 이런한 컴플레인이 발생했을 때 매뉴얼 자체가 없다는 게 안타깝습니다. 그리고 통화할 때 기분 나빠서 다 녹음을 했습니다만, 제가 유숙며 동화한다고 호구는 아닙니다. 또 호텔에 뭔가를 두더내기 위해 진상을 부릴 마음과 시간도 없습니다. 전 그냥 제가 거절 의례 대처와 저를 대하는 태도에 짬 화가 날 뿐입니다. 제가 유숙며 좋게 좋게 통화했던 이유는 전 대학에서 호텔 경영학 전공을 했고, 친구들이 호텔에서 노예처럼 힘들게 일하는 걸 알기에 호텔리어에게 상냥하게 대하고 싶었습니다. 그러나 돌아오는 건 거지와 호구 취급이라니 수습하네요. (호텔 측에서 말씀하신 왜 죽이 얘기하지 않고 체크아웃할 때 얘기했냐고 했던 부분에 대한 설명이 될까요? 전 그냥 얘기 안 하고 가려 했으나 친구들이 그래도 마른 해줘야 한다 해기엔 체크아웃할 때 말씀드린 겁니다. 튀어나온 건 거지와 호구 취급이라니 수습하네요. (호텔 측에서 말씀하신 왜 죽이 얘기하지 않고 체크아웃할 때 얘기했냐고 했던 부분에 대한 설명이 될까요? 전 그냥 얘기 안 하고 가려 했으나 친구들이 그래도 마른 해줘야 한다 해기엔 체크아웃할 때 말씀드린 겁니다. 튀어나온 건 거지와 호구 취급이라니 수습하네요. (호텔 측에서 말씀하신 왜 죽이 얘기하지 않고 체크아웃할 때 얘기했냐고 했던 부분에 대한 설명이 될까요? 전 그냥 얘기 안 하고 가려 했으나 친구들이 그래도 마른 해줘야 한다 해기엔 체크아웃할 때 말씀드린 겁니다. 튀어나온 건 거지와 호구 취급이라니 수습하네요. (호텔 측에서 말씀하신 왜 죽이 얘기하지 않고 체크아웃할 때 얘기했냐고 했던 부분에 대한 설명이 될까요? 전 그냥 얘기 안 하고 가려 했으나 친구들이 그래도 마른 해줘야 한다 해기엔 체크아웃할 때 말씀드린 겁니다. 튀어나온 건 거지와 호구 취급이라니 수습하네요. (호텔 측에서 말씀하신 왜\n",
      "Error at index 1367: 세신이 좋았으나 룸 컨디션이 별로였어요.<end_of_\n",
      "Error at index 1370: 모두 만족스러워요!<end_of\n",
      "Error at index 1374: 적당히 청결이고, 침대가 훌쩍여 목이 불편해요.<end_of_turn>\n",
      "Error at index 1383: 분양형이라 언제 될지 모르는 곳<end_of_turn\n",
      "Error at index 1385: 국내 3대 맛집이라는 것에 숙그하게 됩니다.<end_of_turn>\n",
      "Error at index 1387: 믿고 숙박하는 신라스테이<end_of\n",
      "Error at index 1399: 조식 서비스 꽝입니다~<end_of_turn>\n",
      "Error at index 1406: 최근 펜션 주인이 바뀌어서 그런지 새로운 출발을 알리는 듯 펜션 전체가 산뜻하게 단장되어 있고 복층 구조의 객실과 깨끗한 침구 덕분에 편안하게 지냈습니다. 3개 동이 나란히 있어서 소박한 리조트 같은 분위기에다 바로 앞에는 넓은 무료 공영 주차장까지 있습니다. 룸메이드가 2층에서 너무 고를 고르는 통에 새벽에 잠을 깼는데, 베란다 앞에서 펼쳐지는 통행의 일출은 정말 장관이었습니다. 펜션에 있는 수영장도 좋았지만, 펜션 앞에는 궁촌과 용화 해수욕장을 오가는 레일 바이크를 마음껏 타고 소박한 연평 해변이 있습니다. 한밤중에 여기서 바라보는 하늘에서는 별들이 하염없이 펼쳐집니다. 펜션 바로 옆 바비큐도 좋았고, 인근 대금굴과 운하철도(동심을 일깨우는 ‘은하철도 999’ 노래가 나옵니다. ㅎㅎ), 웅포산 떡봉 계곡 트레킹(천2 용소골 정도 다녀오면 딱입니다.), 삼척 해양 레일 바이크(막판에 김교 선을 긋는 해정 터널이 조명쇼가 인상적입니다. 인기 짱이어서 미리 예약하지 않으면 현장 티켓팅하기 어려워요.), 해양 계이플라이 백(막판에 김교 선을 긋는 해정 터널이 조명쇼가 인상적입니다. 인기 짱이어서 미리 예약하지 않으면 현장 티켓팅하기 어려워요.), 해양 계이플라이 백(막판에 김교 선을 긋는 해정 터널이 조명쇼가 인상적입니다. 인기 짱이어서 미리 예약하지 않으면 현장 티켓팅하기 어려워요.), 해양 계이플라이 백(막판에 김고 선을 긋는 해정 터널이 조명쇼가 인상적입니다. 인기 짱이어서 미리 예약하지 않으면 현장 티켓팅하기 어려워요.), 해양 계이플라이 백(막판에 김고 선을 긋는 해정 터널이 조명쇼가 인상적입니다. 인기 짱이어서 미리 예약하지 않으면 현장 티켓팅하기 어려워요.), 해양 계이플라이 백(막판에 김고 선을 긋는 해정 터널이 조명쇼가 인상적입니다. 인기 짱이어서 미리 예약하지 않으면 현장 티켓팅하기 어려워요.), 해양 계이플라이 백(막판에 김고 선을 긋는 해정 터널이 조명쇼가 인상적입니다. 인기 짱이어서 미리 예약하지 않으면 현장 티켓팅하기 어려워요.), 해양 계이플라이 백(막판에 김고 선을 긋는 해정 터널이 조명쇼가 인상적입니다. 인기 짱이어서 미리 예약하지 않으면 현장 티켓팅하기 어려워요.), 해양 계이플라이 백(막판에 김고 선을 긋는 해정 터널이 조명쇼가 인상적입니다. 인기 짱이어서 미리 예약하지\n",
      "Error at index 1415: 서비스 정말 좋아요:D<end_of_turn\n",
      "Error at index 1418: 최고예요!!<end_\n",
      "Error at index 1431: 뷰 맛집 직원들도 친절해요.<end_of_\n",
      "Error at index 1449: 지하철역이라 찾기 쉬웠고 이용하기 편했어요.<end_of_turn>\n",
      "Error at index 1467: 세상이힐… 힐링에 최고네요. 돌판암물에 꽃이 너무 좋아요~<end_of_turn>\n",
      "Error at index 1470: 비즈니스 용도로 하루 머물기엔 쓸만해요.<end_of_turn\n",
      "Error at index 1472: 주인분들이 친절하시고, 뷰도 너무 좋아요 ㅎㅎ<end_of_turn>\n",
      "Error at index 1489: 직원분들이 친절하시고 좋았어요~<end_of_\n",
      "Error at index 1493: 기대치만큼 딱 충족됨<end_\n",
      "Error at index 1505: 숲으로 꼬불꼬불 가까워서 좋네요^^<end_of\n",
      "Error at index 1513: 좁고 불편한 걸 감안하게 해주는 푸푸 맛집이에요~~<end_of_turn>\n",
      "Error at index 1521: 건물이 무너질 것 같음<end_of_\n",
      "Error at index 1532: 위치가 좋고, 친절하며 깔끔합니다.<end_of_turn>\n",
      "Error at index 1534: 너무너무 편안하고 좋았습니당<end_of_\n",
      "Error at index 1556: 신라스테이 삼성 오픈 축하드립니다!!~~<end_of_turn>\n",
      "Error at index 1562: 젊은 남직원 교육 좀 시켜라.<end_of_turn>\n",
      "Error at index 1579: 깨끗하고 직원들이 친절하네요.<end_of_turn>\n",
      "Error at index 1592: 주차 빼고는 다 좋은 것 같음.<end_of_turn>\n",
      "Error at index 1593: 지한 무드등 뷰이록에 나옴~<end_of_\n",
      "Error at index 1596: 커피 마시러 종종 갑니다.<end_of_turn\n",
      "Error at index 1605: 깨끗하고 친절하며 위치가 좋고<end_of_turn>\n",
      "Error at index 1606: 혼자 지내기 좋아요! 깔끔해요.<end_of_turn\n",
      "Error at index 1611: 아무 때나 가도 괜찮은 시티 뷰.<end_of_turn\n",
      "Error at index 1621: 가격이 조금 높은 왜 만족합니다.<end_of_turn>\n",
      "Error at index 1630: 벌레도 없고 객실이 정말 깔끔하고 화장실도 너무 깨끗해요.<end_of_turn>\n",
      "Error at index 1633: 깨끗하고 가격도 괜찮고 만족해요!<end_of_turn\n",
      "Error at index 1634: 조용히 쉬다가기 좋은 곳 같아요.<end_of_turn>\n",
      "Error at index 1640: 신규 캠핑장이라 깨끗해서 좋았습니다~<end_of_turn\n",
      "Error at index 1641: 객실이 넓고 루프탑 수영장도 깔끔하니 좋았어요옹.<end_of_turn\n",
      "Error at index 1644: 깨끗한데 좀 시끄럽네요.<end_of\n",
      "Error at index 1646: 백화점 이용하기 좋았어요.<end_of_turn>\n",
      "Error at index 1648: 위치가 좋고 방도 깨끗한데 방음이..ㅠㅠ<end_of_turn\n",
      "Error at index 1654: 수영장 낮이스 주차장<end_of\n",
      "Error at index 1660: 단체 숙박도 굿입니다.<end_of_\n",
      "Error at index 1666: 방이 깨끗해서 좋았어요.<end_of_\n"
     ]
    }
   ],
   "source": [
    "def safe_split_newline(x, index):\n",
    "    parts = x.split(\"\\n\")\n",
    "    if len(parts) > 1:\n",
    "        return parts[1]\n",
    "    else:\n",
    "        print(f\"Error at index {index}: {x}\")  # 에러가 발생한 인덱스와 값 출력\n",
    "        return x  # 원래 값 유지 (또는 적절한 처리)\n",
    "\n",
    "for i in range(len(restored_reviews)):\n",
    "    safe_split_newline(restored_reviews[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059421b0-df8e-4d9c-90a5-54c81f7c3c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv', encoding = 'utf-8-sig')\n",
    "\n",
    "submission.to_csv('./submission/augmentation(shuffle)_sampling_submission.csv', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927199d9-7046-4e43-9da4-8d54c5b895a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['output'] = restored_reviews\n",
    "submission['output'] = submission['output'].apply(lambda x: x.split(\"<end_of_turn>\")[0])\n",
    "submission['output'] = submission['output'].apply(lambda x: x.split(\"<end\")[0])\n",
    "submission['output'] = submission['output'].fillna('0')\n",
    "\n",
    "submission.to_csv('./submission/augmentation(shuffle)_sampling_submission.csv', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef7ad4-77f0-45fd-8195-72587856d485",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    test_text = test['input'][i]\n",
    "    sub_text = submission['output'][i]\n",
    "    state = True\n",
    "    \n",
    "    # 먼저 길이를 맞추기 위해 단어 수와 띄어쓰기 조정\n",
    "    while len(sub_text) != len(test_text):\n",
    "        if not state:\n",
    "            break\n",
    "        for idx in range(max(test_text, sub_text)):\n",
    "            if idx == len(test_text):\n",
    "                state = False\n",
    "                break\n",
    "            if (t == ' ') and (s != ' '):\n",
    "                sub_text = sub_text[:idx] + sub_text[idx+1:]\n",
    "            elif (t != ' ') and (s == ' '):\n",
    "                sub_text = sub_text[:idx] + '서' + sub_text[idx:]\n",
    "                \n",
    "            break  # 수정했으므로 다시 검사\n",
    "\n",
    "    # 만약 여전히 길이가 다르면, test 길이에 맞게 자르기\n",
    "    submission['output'][i] = sub_text[:len(test_text)]\n",
    "\n",
    "submission.to_csv('./submission/augmentation(shuffle)_sampling_submission.csv', index = False, encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
